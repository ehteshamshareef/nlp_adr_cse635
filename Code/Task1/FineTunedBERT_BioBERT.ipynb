{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "mlaqQ3wyJooj",
    "outputId": "8e8f5d39-87a4-4ad3-8475-038f59e62aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "191zq3ZErihP"
   },
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# import json\n",
    "# import os\n",
    "# import pprint\n",
    "# import random\n",
    "# import string\n",
    "# import sys\n",
    "# import tensorflow as tf\n",
    "\n",
    "# assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
    "# TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "# print('TPU address is', TPU_ADDRESS)\n",
    "\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "# with tf.compat.v1.Session(TPU_ADDRESS) as session:\n",
    "#   print('TPU devices:')\n",
    "#   pprint.pprint(session.list_devices())\n",
    "\n",
    "  # Upload credentials to TPU.\n",
    "  # with open('/content/adc.json', 'r') as f:\n",
    "  #   auth_info = json.load(f)\n",
    "  # tf.compat.v1.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
    "  # Now credentials are set for all future sessions on this TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PpxHBvE7BI2C"
   },
   "outputs": [],
   "source": [
    "# import torch_xla_py.xla_model as xm\n",
    "\n",
    "# device = xm.xla_device()\n",
    "\n",
    "# xm.optimizer_step(optimizer)\n",
    "# xm.mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "K2ILCuzWJsSG",
    "outputId": "2dfd9a56-2c1c-4ea8-c8a2-9efddf159bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "   \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "SdIfkYpx1zKL",
    "outputId": "96ca2dbe-60be-4044-fcb1-abe8e1f65b0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMzP8K9E2MuU"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t73DuTMg1zzh"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/content/drive/My Drive/task2_en_training.tsv',sep = \"\\t\",encoding = \"ISO-8859-1\")\n",
    "df_test = pd.read_csv('/content/drive/My Drive/task2_en_validation.tsv',sep = \"\\t\",encoding = \"ISO-8859-1\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = df_train[\"tweet\"],df_test[\"tweet\"],df_train[\"class\"],df_test[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "qVhY-qDw9sR1",
    "outputId": "f578ddba-1871-49e2-c10b-ede7dc2e4e1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>344266386467606528</td>\n",
       "      <td>809439366</td>\n",
       "      <td>0</td>\n",
       "      <td>depression hurts, cymbalta can help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>349220537903489025</td>\n",
       "      <td>323112996</td>\n",
       "      <td>0</td>\n",
       "      <td>@jessicama20045 right, but cipro can make thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>351421773079781378</td>\n",
       "      <td>713100330</td>\n",
       "      <td>0</td>\n",
       "      <td>@fibby1123 are you on paxil .. i need help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>326594278472171520</td>\n",
       "      <td>543113070</td>\n",
       "      <td>0</td>\n",
       "      <td>@redicine the lamotrigine and sjs just made ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345567138376994816</td>\n",
       "      <td>138795534</td>\n",
       "      <td>0</td>\n",
       "      <td>have decided to skip my #humira shot today. my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  ...                                              tweet\n",
       "0  344266386467606528  ...                depression hurts, cymbalta can help\n",
       "1  349220537903489025  ...  @jessicama20045 right, but cipro can make thin...\n",
       "2  351421773079781378  ...         @fibby1123 are you on paxil .. i need help\n",
       "3  326594278472171520  ...  @redicine the lamotrigine and sjs just made ch...\n",
       "4  345567138376994816  ...  have decided to skip my #humira shot today. my...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "colab_type": "code",
    "id": "EjYT7jc3J9VH",
    "outputId": "75a13134-e9ac-4105-a464-d5e886bb6487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
      "\u001b[K     |████████████████████████████████| 573kB 2.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 8.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 14.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 30.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=4115d049ee2763322292fdf5de91b3d8265bd106b7dc8db0d9a1c6198c596309\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "4P2EpIe6OCtq",
    "outputId": "3f0d75fd-7c02-491a-bde1-04ec25645218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_pretrained_bert\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "\r",
      "\u001b[K     |██▋                             | 10kB 18.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 30kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 40kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 51kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 61kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 71kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 81kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 92kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 102kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 112kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 122kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 133kB 2.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.38.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.12.47)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.3)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.15.47)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->pytorch_pretrained_bert) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->pytorch_pretrained_bert) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.47->boto3->pytorch_pretrained_bert) (1.12.0)\n",
      "Installing collected packages: pytorch-pretrained-bert\n",
      "Successfully installed pytorch-pretrained-bert-0.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVuv2C1p7UoI"
   },
   "outputs": [],
   "source": [
    "!tar -xzf \"/content/drive/My Drive/biobert_large_v1.1_pubmed.tar.gz\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Ni5LzbZGg16i",
    "outputId": "8a2197d3-c016-4993-d651-8f79c4db17e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbiobert_large\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DsIg_UoKdlqp",
    "outputId": "f8dfdd39-fdc8-40c9-dabd-f8934738260a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch model from configuration: {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 58996\n",
      "}\n",
      "\n",
      "Converting TensorFlow checkpoint from /content/biobert_large/bio_bert_large_1000k.ckpt\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/embeddings/position_embeddings with shape [512, 1024]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 1024]\n",
      "Loading TF weight bert/embeddings/word_embeddings with shape [58996, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/pooler/dense/bias with shape [1024]\n",
      "Loading TF weight bert/pooler/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight global_step with shape []\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "Skipping global_step\n",
      "Save PyTorch model to /content/biobert_large/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pytorch_pretrained_bert.convert_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch\n",
    "path_bin = '/content/biobert_large/pytorch_model.bin'\n",
    "path_bert = '/content/biobert_large/'\n",
    "\n",
    "if (not os.path.exists(path_bin)):\n",
    "  convert_tf_checkpoint_to_pytorch(\n",
    "  path_bert + \"bio_bert_large_1000k.ckpt\",\n",
    "  path_bert + \"config.json\",\n",
    "  path_bert + \"pytorch_model.bin\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DmiCRxVOYvc"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "# print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('/content/biobert_large', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "WXNkQPzkOpdT",
    "outputId": "da479ffa-40aa-47ba-f06c-1b559f71fa56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  depression hurts, cymbalta can help\n",
      "Token IDs: tensor([  101,  7560, 15483,   117, 34577, 10806, 23934,  1169,  1494,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "\n",
    "for sent in X_train:\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 64,           \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt',     \n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(Y_train)\n",
    "\n",
    "\n",
    "print('Original: ', X_train[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "WXuer0n4Osx4",
    "outputId": "4d167792-861c-4473-d762-321387062fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18,489 training samples\n",
      "2,055 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPtEx_ZxOwEx"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VvlrT-I2AaQE",
    "outputId": "e7d85339-79a3-4b0b-cc6a-ada2124e3fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: conda: command not found\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Uf_Jiw2pOzGY",
    "outputId": "1c498653-b963-4e5c-cc13-47e62e33874e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(58996, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertForTokenClassification\n",
    "\n",
    " \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"/content/biobert_large\", \n",
    "    num_labels = 2, \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "colab_type": "code",
    "id": "3tVaUx9gO3ul",
    "outputId": "79d58fdc-54f1-46d5-f642-12bd444d3d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 393 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (58996, 1024)\n",
      "bert.embeddings.position_embeddings.weight               (512, 1024)\n",
      "bert.embeddings.token_type_embeddings.weight               (2, 1024)\n",
      "bert.embeddings.LayerNorm.weight                             (1024,)\n",
      "bert.embeddings.LayerNorm.bias                               (1024,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight        (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.query.bias               (1024,)\n",
      "bert.encoder.layer.0.attention.self.key.weight          (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.key.bias                 (1024,)\n",
      "bert.encoder.layer.0.attention.self.value.weight        (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.value.bias               (1024,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight      (1024, 1024)\n",
      "bert.encoder.layer.0.attention.output.dense.bias             (1024,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight       (1024,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias         (1024,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight          (4096, 1024)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (4096,)\n",
      "bert.encoder.layer.0.output.dense.weight                (1024, 4096)\n",
      "bert.encoder.layer.0.output.dense.bias                       (1024,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                 (1024,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                   (1024,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                (1024, 1024)\n",
      "bert.pooler.dense.bias                                       (1024,)\n",
      "classifier.weight                                          (2, 1024)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxzny_h5O7sb"
   },
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in list(model.named_parameters()) if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in list(model.named_parameters()) if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr = 3e-5, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubIkBtbvPAkU"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 2\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2VRnTgfPDq8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYugwQHtPGl2"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qgi4-WLyRmA_",
    "outputId": "161e0769-f47b-4ac3-d2eb-9a14940af5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    578.    Elapsed: 0:01:26.\n",
      "  Batch    80  of    578.    Elapsed: 0:02:51.\n",
      "  Batch   120  of    578.    Elapsed: 0:04:17.\n",
      "  Batch   160  of    578.    Elapsed: 0:05:42.\n",
      "  Batch   200  of    578.    Elapsed: 0:07:07.\n",
      "  Batch   240  of    578.    Elapsed: 0:08:32.\n",
      "  Batch   280  of    578.    Elapsed: 0:09:57.\n",
      "  Batch   320  of    578.    Elapsed: 0:11:22.\n",
      "  Batch   360  of    578.    Elapsed: 0:12:47.\n",
      "  Batch   400  of    578.    Elapsed: 0:14:12.\n",
      "  Batch   440  of    578.    Elapsed: 0:15:36.\n",
      "  Batch   480  of    578.    Elapsed: 0:17:01.\n",
      "  Batch   520  of    578.    Elapsed: 0:18:26.\n",
      "  Batch   560  of    578.    Elapsed: 0:19:51.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epcoh took: 0:20:29\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 0.16\n",
      "  Validation took: 0:00:45\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of    578.    Elapsed: 0:01:25.\n",
      "  Batch    80  of    578.    Elapsed: 0:02:50.\n",
      "  Batch   120  of    578.    Elapsed: 0:04:14.\n",
      "  Batch   160  of    578.    Elapsed: 0:05:39.\n",
      "  Batch   200  of    578.    Elapsed: 0:07:04.\n",
      "  Batch   240  of    578.    Elapsed: 0:08:29.\n",
      "  Batch   280  of    578.    Elapsed: 0:09:54.\n",
      "  Batch   320  of    578.    Elapsed: 0:11:19.\n",
      "  Batch   360  of    578.    Elapsed: 0:12:44.\n",
      "  Batch   400  of    578.    Elapsed: 0:14:08.\n",
      "  Batch   440  of    578.    Elapsed: 0:15:33.\n",
      "  Batch   480  of    578.    Elapsed: 0:16:58.\n",
      "  Batch   520  of    578.    Elapsed: 0:18:23.\n",
      "  Batch   560  of    578.    Elapsed: 0:19:47.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epcoh took: 0:20:25\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 0.18\n",
      "  Validation took: 0:00:45\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:42:24 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "      \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = torch.flatten(batch[2].to(device))\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "RkUVqEEwRvgL",
    "outputId": "714648e7-025c-4f23-8ab8-6533af5fc95a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0:20:29</td>\n",
       "      <td>0:00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0:20:25</td>\n",
       "      <td>0:00:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.22         0.16           0.94       0:20:29         0:00:45\n",
       "2               0.11         0.18           0.94       0:20:25         0:00:45"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFN_Y_STVvRn"
   },
   "outputs": [],
   "source": [
    "\n",
    "sentences = X_test.values\n",
    "labels = Y_test.values\n",
    "\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "\n",
    "for sent in sentences:\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 64,           \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt',     \n",
    "                   )\n",
    "    \n",
    "   \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "DBXylfPYWThx",
    "outputId": "a9576a46-ee07-48c1-ad35-855a4a7ebdc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 5,134 test sentences...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ToeLxrJAWk1q"
   },
   "outputs": [],
   "source": [
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9NeR4zdRWmV-",
    "outputId": "d055a7aa-26c4-42c4-918a-14b97faca7d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96      4541\n",
      "           1       0.70      0.56      0.63       593\n",
      "\n",
      "    accuracy                           0.92      5134\n",
      "   macro avg       0.82      0.77      0.79      5134\n",
      "weighted avg       0.92      0.92      0.92      5134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print (sklearn.metrics.classification_report(flat_predictions, flat_true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "bzuDsuWYtqcn",
    "outputId": "a56dd81b-62da-4661-e9b5-b0b7cb64df97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m'My Drive'\u001b[0m/\n",
      "/content/drive/My Drive\n"
     ]
    }
   ],
   "source": [
    "%ls\n",
    "%cd \"/content/drive/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "N3r4y9SitgrF",
    "outputId": "a14c4dc7-8f73-4038-8701-2c2b8cd2120b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/vocab.txt',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "output_dir = './model_save/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  \n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLOoksnVt7BO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Task1_FineTunedBERT_BioBERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
